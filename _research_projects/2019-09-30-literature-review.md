---
layout: post
title:  "Review: Biological plausibility of sparse algorithms in computer vision"
date:   2019-09-30 00:00:00 +0200
categories: research_project
logo_url: /assets/icons/uvalogo.jpg
---
As part of my master's degree I wrote a review on machine learning algorithms in computer vision and how different methods of enforcing sparsity in these models relate to the visual systems in the cortex. 

For this review, I studied papers from both classic Machine Learning literature and Neuroscience research. I related findings in Computer Vision models to physiological evidence in primate and rodent vision systems. This work was compelling from both sides of the stories, as many of the advances of early Machine Learning research are related to our knowledge of brain functioning and in converse current Neuroscience research is making progress through the use of novel Machine Learning models. I was supervised by Shirin Dora at the Swammerdam Institute for Life Science, University of Amsterdam. The review is available [here](/assets/reports/ReviewGrzelkowski.pdf). 

#### Abstract
Neuroscience and artificial intelligence have a long history of contributing to each otherâ€™s progress. Early models in computer vision research were heavily influenced by our knowledge of early visual processing in the cortex. Sparseness quickly proved to be an effective constraint to produce powerful algorithms that were able to perform object classification. In this review, we present different methods for enforcing sparse populations in computer vision and how they affect network properties that relate to similarity with the sensory processing stream in the cortex. We show that different forms of regularization can be an effective way of promoting sparseness in neural network populations. L1 and L2 regularization, common ways of restraining the size of network parameters, create distributions of network activity that are comparable to those found in biological networks. In particular, L1 regularization can be compared to the evolutionary pressure on cortical processing to reduce energy consumption. Another form of regularization, dropout, randomly removes units in the network during training. This method also results in a sparse population of units during the testing phase. We argue that the introduction of dropout can be compared to the noisy conditions of natural sensory processing. The last spars algorithm we discuss relies on a specific form of activation function, the rectifier linear unit (ReLU). The ReLU resembles response characteristics of neurons in the early visual cortex and is therefore of particular interest in investigating the effects on network activity. In addition to producing sparse populations, all discussed methods affect the feature selectivity of units in the networks. As there is no consensus in the literature on the interaction between sparseness and feature selectivity we highlight the evidence sparse algorithms contribute to this discussion. This review offers insights into the interaction of computer vision and neuroscience and demonstrates gaps in our understanding of the connections between the two that need to be further investigated. 